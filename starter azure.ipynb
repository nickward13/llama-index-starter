{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Azure Open AI\n",
    "\n",
    "Based on https://docs.llamaindex.ai/en/stable/getting_started/starter_example/\n",
    "\n",
    "LlamaIndex is available in Python (these docs) and Typescript. If you're not sure where to start, we recommend reading how to read these docs which will point you to the right place based on your experience level.\n",
    "\n",
    "## 30 second quickstart\n",
    "\n",
    "Set an environment variable called OPENAI_API_KEY with an OpenAI API key. \n",
    "\n",
    "Install the Python library:\n",
    "\n",
    "```bash\n",
    "pip install llama-index-llms-azure-openai\n",
    "pip install llama-index\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get environment variables\n",
    "\n",
    "OPENAI_API_VERSION: set this to 2023-07-01-preview This may change in the future.\n",
    "AZURE_OPENAI_ENDPOINT: your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/\n",
    "OPENAI_API_KEY: your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_MODEL = os.getenv(\"AZURE_OPENAI_MODEL\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "AZURE_OPENAI_API_VERSION = \"2023-07-01-preview\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a LlamaIndex llm object using AzureOpenAI and query it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sun is shining brightly. The warmth of the sun's rays gently caresses the skin, providing a comfortable and soothing sensation. Fluffy white clouds drift lazily across the vast expanse, creating an ever-changing tapestry that captures the imagination. The light breeze carries with it the fresh scent of nature, perhaps the fragrance of blooming flowers or the clean smell of a recent rain. The sound of birds singing melodically in the distance adds to the serene atmosphere, creating a sense of peace and tranquility. It's the kind of day that invites outdoor activities, from a leisurely walk in the park to a spirited game of frisbee, or simply lying on the grass and watching the clouds roll by. The beautiful blue sky is a classic symbol of hope and possibility, reminding us of the beauty that exists in the world and encouraging us to take a moment to appreciate the simple joys of life.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    engine=AZURE_OPENAI_MODEL,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    model=AZURE_OPENAI_MODEL,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "response = llm.complete(\"The sky is a beautiful blue and\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create or retrieve the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index from storage\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.core import (SimpleDirectoryReader, VectorStoreIndex)\n",
    "from llama_index.core import load_index_from_storage, StorageContext\n",
    "\n",
    "def create_index(persist_dir):\n",
    "    print(\"Creating index from data\")\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    index.storage_context.persist(persist_dir=persist_dir)\n",
    "    return index\n",
    "\n",
    "def load_index(persist_dir):\n",
    "    print(\"Loading index from storage\")\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "    return index\n",
    "\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    index = create_index()\n",
    "else:\n",
    "    index = load_index(PERSIST_DIR)\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create class to instantiate a custom query engine which uses AzureOpenAI as llm provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.response_synthesizers import BaseSynthesizer\n",
    "\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "class RAGStringQueryEngine(CustomQueryEngine):\n",
    "    \"\"\"RAG String Query Engine.\"\"\"\n",
    "\n",
    "    retriever: BaseRetriever\n",
    "    response_synthesizer: BaseSynthesizer\n",
    "    llm: AzureOpenAI\n",
    "    qa_prompt: PromptTemplate\n",
    "\n",
    "    def custom_query(self, query_str: str):\n",
    "        nodes = self.retriever.retrieve(query_str)\n",
    "\n",
    "        context_str = \"\\n\\n\".join([n.node.get_content() for n in nodes])\n",
    "        response = self.llm.complete(\n",
    "            qa_prompt.format(context_str=context_str, query_str=query_str)\n",
    "        )\n",
    "\n",
    "        return str(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever()\n",
    "synthesizer = get_response_synthesizer(response_mode=\"compact\")\n",
    "\n",
    "query_engine = RAGStringQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=synthesizer,\n",
    "    llm=llm,\n",
    "    qa_prompt=qa_prompt,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the LlamaIndex using Azure Open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The universally understood truth is that a single man in possession of a good fortune must be in want of a wife.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What truth is universally understood?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn on logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the model for later retrieval\n",
    "\n",
    "By default, the data you just loaded is stored in memory as a series of vector embeddings. You can save time (and requests to OpenAI) by saving the embeddings to disk. That can be done with this line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
